#!/usr/bin/env bash
set -e

_add_extra_hosts() {
  IFS=' ' read -r -a EXTRA_HOSTS_ARRAY <<< "$EXTRA_HOSTS"
  for i in "${!EXTRA_HOSTS_ARRAY[@]}"; do
    extra_ip="$(cut -d ':' -f1 <<<${EXTRA_HOSTS_ARRAY[$i]})"
    extra_hostname="$(cut -d ':' -f2 <<<${EXTRA_HOSTS_ARRAY[$i]})"
    cat >> /etc/hosts <<EOF
${extra_ip} ${extra_hostname}
EOF
  done
  cat /etc/hosts
}

_gfs_init_fstab() {
  if [[ ! -f /etc/fstab ]]; then
    cat > /etc/fstab << EOF
### <server>:</remote/export> </local/directory> <glusterfs> defaults,_netdev 0 0
EOF
  fi
}

_gfs_export_mounts() {
  IFS=':' read -r -a MNT_SERVER_ARRAY <<< "$GFS_SERVER_DIRS"
  IFS=':' read -r -a MNT_CLIENT_ARRAY <<< "$GFS_CLIENT_DIRS"
  local gfs_server=$(echo $GFS_SERVERS | cut -d ' ' -f 1)
  for i in "${!MNT_CLIENT_ARRAY[@]}"; do
    if [[ ! -d ${MNT_CLIENT_ARRAY[$i]} ]]; then
      mkdir -p ${MNT_CLIENT_ARRAY[$i]}
    fi
    if grep -q ${MNT_CLIENT_ARRAY[$i]} /etc/fstab; then
      echo "### INFO: fstab entry for ${MNT_SERVER_ARRAY[$i]} already exists ###"
    else
      cat >> /etc/fstab <<EOF
${gfs_server}:${MNT_SERVER_ARRAY[$i]} ${MNT_CLIENT_ARRAY[$i]} glusterfs defaults,_netdev,log-level=WARNING,log-file=/var/log/gluster.log 0 0
EOF
    fi
  done
  cat /etc/fstab
}

_gfs_mount_info() {
  IFS=':' read -r -a MNT_CLIENT_ARRAY <<< "$GFS_CLIENT_DIRS"
  for gfs_mount in "${MNT_CLIENT_ARRAY[@]}"; do
    echo '### Info for: '$gfs_mount' ###'
    mount | grep $gfs_mount
    df -h $gfs_mount
  done
}

# start sshd server
_sshd_host() {
  if [ ! -d /var/run/sshd ]; then
    mkdir /var/run/sshd
    ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key -N ''
  fi
  /usr/sbin/sshd
}

# setup root ssh to be passwordless
_ssh_root() {
  mkdir -p ~/.ssh
  chmod 0700 ~/.ssh
  if [[ ! -f /.secret/root_ssh_controller/id_rsa ]]; then
    ssh-keygen -b 2048 -t rsa -f ~/.ssh/id_rsa -q -N "" -C "$(whoami)@$(hostname)-$(date -I)"
    cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys
    chmod 0640 ~/.ssh/authorized_keys
    cat >> ~/.ssh/config <<EOF
Host *
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null
  LogLevel QUIET
EOF
    chmod 0644 ~/.ssh/config
    mkdir -p /.secret/root_ssh_controller
    cp ~/.ssh/id_rsa ~/.ssh/id_rsa.pub /.secret/root_ssh_controller
  else
    cp /.secret/root_ssh_controller/id_rsa ~/.ssh/id_rsa
    cp /.secret/root_ssh_controller/id_rsa.pub ~/.ssh/id_rsa.pub
    cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys
    chmod 0640 ~/.ssh/authorized_keys
    cat >> ~/.ssh/config <<EOF
Host *
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null
  LogLevel QUIET
EOF
    chmod 0644 ~/.ssh/config
  fi
}

# setup worker ssh to be passwordless
_ssh_worker() {
  if [[ ! -f /home/worker/.ssh/id_rsa ]]; then
    if [[ ! -d /home/worker ]]; then
      mkdir -p /home/worker
      chown -R worker:worker /home/worker
    fi
    cat > /home/worker/setup-worker-ssh.sh <<'EOF2'
mkdir -p ~/.ssh
chmod 0700 ~/.ssh
ssh-keygen -b 2048 -t rsa -f ~/.ssh/id_rsa -q -N "" -C "$(whoami)@$(hostname)-$(date -I)"
cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys
chmod 0640 ~/.ssh/authorized_keys
cat >> ~/.ssh/config <<EOF
Host *
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null
  LogLevel QUIET
EOF
chmod 0644 ~/.ssh/config
cd ~/
tar -czvf ~/worker-secret.tar.gz .ssh
cd -
EOF2
    chmod +x /home/worker/setup-worker-ssh.sh
    chown worker: /home/worker/setup-worker-ssh.sh
    sudo -u worker /home/worker/setup-worker-ssh.sh
  else
    echo "### INFO: worker /home/worker/.ssh/id_rsa already exists ###"
  fi
}

# start munge and generate key
_munge_start() {
  chown -R munge: /etc/munge /var/lib/munge /var/log/munge /var/run/munge
  chmod 0700 /etc/munge
  chmod 0711 /var/lib/munge
  chmod 0700 /var/log/munge
  chmod 0755 /var/run/munge
  /sbin/create-munge-key -f
  sudo -u munge /sbin/munged
  munge -n
  munge -n | unmunge
  remunge
}

# copy secrets to /.secret directory for other nodes
_copy_secrets() {
  if [[ ! -f /.secret/worker-secret.tar.gz ]]; then
    cp /home/worker/worker-secret.tar.gz /.secret/worker-secret.tar.gz
  fi
  if [[ ! -f /.secret/setup-worker-ssh.sh ]]; then
    cp /home/worker/setup-worker-ssh.sh /.secret/setup-worker-ssh.sh
  fi
  if [[ ! -f /.secret/munge.key ]]; then
    cp /etc/munge/munge.key /.secret/munge.key
  fi
  rm -f /home/worker/worker-secret.tar.gz
  rm -f /home/worker/setup-worker-ssh.sh
}

# generate slurm.conf
_generate_slurm_conf() {
  cat > /etc/slurm/slurm.conf <<EOF
#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=$CLUSTER_NAME
ControlMachine=$CONTROL_MACHINE
#ControlAddr=
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=$SLURMCTLD_PORT
SlurmdPort=$SLURMD_PORT
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
#PluginDir=
#FirstJobId=
ReturnToService=0
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
#SelectType=select/linear
FastSchedule=1
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
#JobAcctGatherFrequency=30
#
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=$ACCOUNTING_STORAGE_HOST
AccountingStoragePort=$ACCOUNTING_STORAGE_PORT
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStorageUser=
#
# COMPUTE NODES
EOF
  IFS=' ' read -r -a COMPUTE_NODE_ARRAY <<< "$COMPUTE_NODES"
  for compute_node in "${COMPUTE_NODE_ARRAY[@]}"; do
    cat >> /etc/slurm/slurm.conf <<EOF
NodeName=${compute_node} RealMemory=$MEM CPUs=$CPUS State=UNKNOWN
EOF
  done
  cat >> /etc/slurm/slurm.conf <<EOF
PartitionName=$PARTITION_NAME Nodes=ALL Default=YES MaxTime=INFINITE State=UP
EOF
}

# run slurmctld
_slurmctld() {
  if $USE_SLURMDBD; then
    echo -n "cheking for slurmdbd.conf"
    while [ ! -f /.secret/slurmdbd.conf ]; do
      echo -n "."
      sleep 1
    done
    echo ""
    sleep 1s
  fi
  mkdir -p /var/spool/slurm/ctld \
    /var/spool/slurm/d \
    /var/log/slurm
  chown -R slurm: /var/spool/slurm/ctld \
    /var/spool/slurm/d \
    /var/log/slurm
  touch /var/log/slurmctld.log
  chown slurm: /var/log/slurmctld.log
  if [[ ! -f /home/config/slurm.conf ]]; then
    echo "### generate slurm.conf ###"
    _generate_slurm_conf
  else
    echo "### use provided slurm.conf ###"
    cp /home/config/slurm.conf /etc/slurm/slurm.conf
  fi
  sacctmgr -i add cluster ${CLUSTER_NAME}
  sleep 2s
  /usr/sbin/slurmctld
  cp -f /etc/slurm/slurm.conf /.secret/
#  /usr/sbin/slurmctld -Dvvv ### enable for debugging (comment out call above)
}

_node_name_etc_hosts() {
#  local node_entry=$(cat /etc/hosts | grep $(hostname))
#  local new_node_entry=$node_entry' '${NODE_NAME}
  cp /etc/hosts /tmp/hosts
  sed -i "/$(cat /etc/hosts | grep $(hostname))/c\\$MY_HOST_ENTRY" /tmp/hosts
  cat /tmp/hosts > /etc/hosts
  cat > /.secret/etc_hosts <<EOF
${MY_HOST_ENTRY}
EOF
  cat > /.secret/controller_host_entry <<EOF
export CONTROLLER_HOST_ENTRY='${MY_HOST_ENTRY}'
EOF
  echo "### cat /etc/hosts ###"
  cat /etc/hosts
}

### Check for secret files from a prior run
_check_for_secret_files() {
  if [[ -f /.secret/munge.key ]]; then
    rm -f /.secret/munge.key
  fi
  if [[ -f /.secret/slurmdbd.conf ]]; then
    rm -f /.secret/slurmdbd.conf
  fi
  if [[ -f /.secret/slurm.conf ]]; then
    rm -f /.secret/slurm.conf
  fi
  if [[ -f /.secret/controller_host_entry ]]; then
    rm -f /.secret/controller_host_entry
  fi
  if [[ -f /.secret/etc_hosts ]]; then
    rm -f /.secret/etc_hosts
  fi
}

### monitor /.secret/etc_hosts for new entries
_monitor_etc_hosts() {
  if ! grep -q ${MY_HOST_ENTRY} /.secret/etc_hosts; then
    cat >> /.secret/etc_hosts <<EOF
${MY_HOST_ENTRY}
EOF
  fi
  while read host_entry; do
    if ! grep -q ${host_entry} /etc/hosts; then
      cat >> /etc/hosts <<EOF
${host_entry}
EOF
    fi
  done < <(cat /.secret/etc_hosts)

  inotifywait -mr --timefmt '%d/%m/%y %H:%M' --format '%T %w %f' /.secret/etc_hosts | \
  while read date time dir file; do
    if ! grep -q ${MY_HOST_ENTRY} /.secret/etc_hosts; then
      cat >> /.secret/etc_hosts <<EOF
${MY_HOST_ENTRY}
EOF
    fi
    while read host_entry; do
      if ! grep -q ${host_entry} /etc/hosts; then
        cat >> /etc/hosts <<EOF
${host_entry}
EOF
      fi
    done < <(cat /.secret/etc_hosts)
  done
}

### move module files from gfs volume to inside each container
_move_module_files() {
  if [ -d "/modules" ]; then
    echo "Copying modules..."
    cp -R /modules /opt/apps/Linux
  fi 
  if [ -d "/modulefiles" ]; then
    echo "Copying modulefiles..."
    mkdir -p /opt/apps/modulefiles/Linux
    cp -R /modulefiles/* /opt/apps/modulefiles/Linux
  fi
}

### main ###
_add_extra_hosts

gfs_server=$(echo $GFS_SERVERS | cut -d ' ' -f 1)
echo "connecting to ${gfs_server}"
until [ $(ping ${gfs_server} -c 3 2>&1 >/dev/null)$? ]; do
  echo -n "."
  sleep 1
done
sleep 1s

_gfs_init_fstab
_gfs_export_mounts
mount -a
sleep 1s
_gfs_mount_info

#_move_module_files
echo "check 1"
_check_for_secret_files
export CONTROL_MACHINE=$(hostname)
export MY_HOST_ENTRY=$(cat /etc/hosts | grep $(hostname))' '${NODE_NAME}
echo "check 2"
_node_name_etc_hosts
echo "check 3"
_sshd_host
echo "check 4"
_ssh_root
echo "check 5"
_ssh_worker
echo "check 6"
_munge_start
echo "check 7"
_copy_secrets
echo "check 8"
_slurmctld
echo "check 9"
_monitor_etc_hosts
tail -f /dev/null
